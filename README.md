<div style="text-align: center">
<img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_banner_dark.png">
</div>

# TRL - Transformer Reinforcement Learning
> ê°•í™” í•™ìŠµì„ ì ìš©í•œ ì „ì²´ ìŠ¤íƒ íŠ¸ëœìŠ¤í¬ë¨¸ ì–¸ì–´ ëª¨ë¸.

<p align="center">
    <a href="https://github.com/huggingface/trl/blob/main/LICENSE">
        <img alt="License" src="https://img.shields.io/github/license/huggingface/trl.svg?color=blue">
    </a>
    <a href="https://huggingface.co/docs/trl/index">
        <img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/trl/index.svg?down_color=red&down_message=offline&up_message=online">
    </a>
    <a href="https://github.com/huggingface/trl/releases">
        <img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/trl.svg">
    </a>
</p>

ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€?
<div style="text-align: center">
<img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png">
</div>


`trl`ì€ ë³€ì••ê¸° ì–¸ì–´ ëª¨ë¸ê³¼ ì•ˆì •ì ì¸ í™•ì‚° ëª¨ë¸ì„ ê°•í™” í•™ìŠµìœ¼ë¡œ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•œ ì „ì²´ ìŠ¤íƒ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ëŠ” Supervised Fine-tuning ë‹¨ê³„(SFT), Reward Modeling ë‹¨ê³„(RM)ì—ì„œ Proximal Policy Optimization (PPO) ë‹¨ê³„ê¹Œì§€ ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ğŸ¤— Hugging Faceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë¯€ë¡œ, ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì€ transformersë¥¼ í†µí•´ ì§ì ‘ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ëŒ€ë¶€ë¶„ì˜ ë””ì½”ë” ì•„í‚¤í…ì²˜ì™€ ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ê°€ ì§€ì›ë©ë‹ˆë‹¤. ì˜ˆì œ ì½”ë“œ ì¡°ê°ê³¼ ì´ ë„êµ¬ë“¤ì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì€ ë¬¸ì„œë‚˜ `examples/` í´ë”ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.


**Highlights:**

- [`SFTTrainer`]: ì‚¬ìš©ì ì§€ì • ë°ì´í„°ì…‹ì—ì„œ ì–¸ì–´ ëª¨ë¸ì´ë‚˜ ì–´ëŒ‘í„°ë¥¼ ì‰½ê²Œ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ íŠ¸ë ˆì´ë„ˆ ì£¼ë³€ì˜ ê°€ë²¼ìš°ë©° ì¹œìˆ™í•œ ë˜í¼ì…ë‹ˆë‹¤.
- [`RewardTrainer`]: ì¸ê°„ì˜ ì„ í˜¸ë„(ë³´ìƒ ëª¨ë¸ë§)ì— ë”°ë¼ ì–¸ì–´ ëª¨ë¸ì„ ì‰½ê²Œ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ íŠ¸ë ˆì´ë„ˆ ì£¼ë³€ì˜ ê°€ë²¼ìš´ ë˜í¼ì…ë‹ˆë‹¤.
- [`PPOTrainer`]: ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ PPO íŠ¸ë ˆì´ë„ˆë¡œ, ì–¸ì–´ ëª¨ë¸ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ (ì§ˆë¬¸, ì‘ë‹µ, ë³´ìƒ) ì‚¼ì¤‘í•­ë§Œ í•„ìš”í•©ë‹ˆë‹¤.
- [`AutoModelForCausalLMWithValueHead & AutoModelForSeq2SeqLMWithValueHead`]: ê°•í™” í•™ìŠµì—ì„œ ê°’ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê° í† í°ì— ëŒ€í•œ ì¶”ê°€ ìŠ¤ì¹¼ë¼ ì¶œë ¥ì„ ê°€ì§„ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì…ë‹ˆë‹¤.
- [ì˜ˆì‹œ]: BERT ê°ì • ë¶„ë¥˜ê¸°ë¡œ ê¸ì •ì ì¸ ì˜í™” ë¦¬ë·°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ GPT2ë¥¼ í›ˆë ¨ì‹œí‚¤ê¸°, ì–´ëŒ‘í„°ë§Œì„ ì‚¬ìš©í•œ ì „ì²´ RLHF, GPT-jë¥¼ ëœ ë…ì„± ìˆê²Œ í›ˆë ¨ì‹œí‚¤ê¸°, Stack-Llama ì˜ˆì‹œ ë“±.

## PPO ì‘ë™ ë°©ì‹
ì–¸ì–´ ëª¨ë¸ì„ PPOë¥¼ í†µí•´ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì€ ëŒ€ëµ ì„¸ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

1. **ë¡¤ì•„ì›ƒ**: ì–¸ì–´ ëª¨ë¸ì€ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë°˜ì‘ì´ë‚˜ ì—°ì†ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì¥ì˜ ì‹œì‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **í‰ê°€**: ì§ˆë¬¸ê³¼ ë°˜ì‘ì€ í•¨ìˆ˜, ëª¨ë¸, ì¸ê°„ì˜ í”¼ë“œë°± ë˜ëŠ” ê·¸ë“¤ì˜ ì¡°í•© ë“±ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€ ì´ ê³¼ì •ì´ ê° ì§ˆë¬¸/ë°˜ì‘ ìŒì— ëŒ€í•œ ìŠ¤ì¹¼ë¼ ê°’ì„ ì‚°ì¶œí•´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
3. **ìµœì í™”**: ì´ê²ƒì€ ê°€ì¥ ë³µì¡í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. ìµœì í™” ë‹¨ê³„ì—ì„œ ì§ˆë¬¸/ë°˜ì‘ ìŒì€ ì‹œí€€ìŠ¤ ë‚´ì˜ í† í°ì˜ ë¡œê·¸-í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ ì¤‘ì¸ ëª¨ë¸ê³¼ ë³´í†µ ë¯¸ì„¸ ì¡°ì • ì „ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì¸ ì°¸ì¡° ëª¨ë¸ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ë‘ ì¶œë ¥ ì‚¬ì´ì˜ KL-ë°œì‚°ì€ ìƒì„±ëœ ë°˜ì‘ì´ ì°¸ì¡° ì–¸ì–´ ëª¨ë¸ì—ì„œ ë„ˆë¬´ ë©€ë¦¬ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì¶”ê°€ ë³´ìƒ ì‹ í˜¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ í™œì„± ì–¸ì–´ ëª¨ë¸ì€ PPOë¡œ í›ˆë ¨ë©ë‹ˆë‹¤.

ì•„ë˜ì˜ ìŠ¤ì¼€ì¹˜ì—ì„œ ì´ ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤:

<div style="text-align: center">
<img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_overview.png" width="800">
<p style="text-align: center;"> <b>Figure:</b> Sketch of the workflow. </p>
</div>

## Installation

### Python package
Install the library with pip:
```bash
pip install trl
```

### From source
If you want to run the examples in the repository a few additional libraries are required. Clone the repository and install it with pip:
```bash
git clone https://github.com/huggingface/trl.git
cd trl/
pip install .
```

If you wish to develop TRL, you should install in editable mode:
```bash
pip install -e .
```

## How to use

### `SFTTrainer`

This is a basic example on how to use the `SFTTrainer` from the library. The `SFTTrainer` is a light wrapper around the `transformers` Trainer to easily fine-tune language models or adapters on a custom dataset.

```python
# imports
from datasets import load_dataset
from trl import SFTTrainer

# get dataset
dataset = load_dataset("imdb", split="train")

# get trainer
trainer = SFTTrainer(
    "facebook/opt-350m",
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=512,
)

# train
trainer.train()
```

### `RewardTrainer`

This is a basic example on how to use the `RewardTrainer` from the library. The `RewardTrainer` is a wrapper around the `transformers` Trainer to easily fine-tune reward models or adapters on a custom preference dataset.

```python
# imports
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from trl import RewardTrainer

# load model and dataset - dataset needs to be in a specific format
model = AutoModelForSequenceClassification.from_pretrained("gpt2", num_labels=1)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

...

# load trainer
trainer = RewardTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
)

# train
trainer.train()
```

### `PPOTrainer`

This is a basic example on how to use the `PPOTrainer` from the library. Based on a query the language model creates a response which is then evaluated. The evaluation could be a human in the loop or another model's output.

```python
# imports
import torch
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
from trl.core import respond_to_batch

# get models
model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
model_ref = create_reference_model(model)

tokenizer = AutoTokenizer.from_pretrained('gpt2')

# initialize trainer
ppo_config = PPOConfig(
    batch_size=1,
)

# encode a query
query_txt = "This morning I went to the "
query_tensor = tokenizer.encode(query_txt, return_tensors="pt")

# get model response
response_tensor  = respond_to_batch(model, query_tensor)

# create a ppo trainer
ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)

# define a reward for response
# (this could be any reward such as human feedback or output from another model)
reward = [torch.tensor(1.0)]

# train model for one step with ppo
train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)
```

## References

### ê·¼ì ‘ ì •ì±… ìµœì í™”
PPO êµ¬í˜„ì€ ëŒ€ë¶€ë¶„ D. Ziegler ì™¸ì˜ ë…¼ë¬¸ **"ì¸ê°„ì˜ ì„ í˜¸ë„ì—ì„œ ì–¸ì–´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •"**ì—ì„œ ì†Œê°œëœ êµ¬ì¡°ë¥¼ ë”°ë¦…ë‹ˆë‹¤. [ë…¼ë¬¸, ì½”ë“œ].

### ì–¸ì–´ ëª¨ë¸
ì–¸ì–´ ëª¨ë¸ì€ ğŸ¤— Hugging Faceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## Citation

```bibtex
@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}
```
